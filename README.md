# Overview

Regular updating of data requires the following steps.

First, source data the collection of which we've not yet automated:

    python runner.py getmanual

Second, source data we can collect automatically:

    python runner.py getauto

Finally, run all the importers for which there is fresh data, in the
correct order (successful imports are logged to `log.json`):

    python runner.py runimporters

You can also store data in bigquery:

    python runner.py bigquery

And update indexes and materialised views:

    python runner.py updatedb

## Setup

Install python dependencies:

    pip install -r requirements.txt

Set up environment variables:

    OPENP_PYTHON=/webapps/openprescribing/.venv/bin/python"
    OPENP_FRONTEND_APP_DIR=/webapps/openprescribing/openprescribing
    OPENP_DATA_PYTHON=/home/hello/openprescribing-data/.venv/bin/python"
    OPENP_DATA_BASEDIR=/home/hello/openprescribing-data/data

For bigquery-loading support, add the following variable to your environment:

    GOOGLE_APPLICATION_CREDENTIALS=<path-to-credentials.json>

...where `credentials.json` is the output of

   pass show google-service-accounts/bigquery



## How it works

`manifest.json` contains a list of sources that we use in
OpenPrescribing, with descriptions and metadata.

**id** *(required)*: a unique id for this source
**title** *(required)*: short title for the source
**description** *(required)*: description of the source, including important features, rationale for including it in OpenPrescribing, etc
**fetcher**: The name of a python script, which should be placed in the `fetchers/` directory, which gets data for this source. Fetchers should be idempotent. When run, if a fetcher finds new data, it should place the new data in a timestamped folder at `data/<id>/<year>_<month>`.
**importers**: a list of importers, each elemnt of which should be the name of a Django management command (plus switches) in the main app which knows how to import this data. The command must have a `--filename` switch, and the `importer` definition must include a regex as its value which is expected to match the filename
**depends_on**: a list of source ids which should be imported before this source can be imported.
**index_url**: a webpage where the latest version of the dataset can be found by a user
**urls**: A dictionary of URLs where previous data has been found. This is informational, to help a user hunt down the latest version
**tags** *(required)*: a list of tags. Only sources tagged `core_data` are considered manual sources (see below). Otherwise tags are currently just informational
**publication_schedule**: a human-readable string giving the expected publication schedule
**publication_lag**: a human-readable string describing how long after the reporting date the dataset is published

A source without `fetchers`, and with the `core_data` tag, is deemed a
manual source, and therefore appears in the prompt list generated by
`python runner.py getmanual`.

Finally, all the raw data is stored in Google BigQuery.
